Prathyoosha Chaya, CS 163 DATA STRUCTURES, Prof. Karla Fant
Efficiency Review 3
The overlying abstract data type used in this program was a hash table, and an relative linked list of arrays to perform any collision resulution, in the case where multiple items will be stored in the same index. In this scenario, a relative list is definitely the right choice to make in creating the dictionary in use here – as the Pokedex should not have any "holes" in the data, and if an entry (pokemon term definition) is removed from the deck or added, it is relative to other members. The underlying data structures used were a struct entry type to create the data classification within the node and to allow the application to have its own user-filled Pokemon entry to communicate to the ADT, as well as a struct node type. Conceptually, the idea of having the underlying data form structures made sense. The data types could be easily grouped together and is easily accessible by the node class. By having the entry data and node as separate classes, the construction and destruction of these objects could be neatly managed.
To store the data about the Pokedex dictionary the hash function was used to first find the entry's index number, then store this into the hash table. An array have allowed for easy access and to avoid (extra) traversals in the cases of removing and retrieving items. With tha hash table, all of the data for adding and removing a pokedex item is O(1) unless if  there is a collision, in which case the efficiency is O(chain length). The use of  the hash table is very efficient when compared to using a linear linked list or other type of linked list because of the lack of regular traversals, however it does have a limitation in that the search keyword for retrieval or removing has to match exactly to the items in the hash table. This is slightly resolved with the use of the convert_to_upper function. This ultimately provides an efficient O(1) adding and removing process given no collisions. 
However, the case for no collisions is not reasonable, and in this case with a very small hash table utilized with a size of 37, there are bound to be collisions, especially with larger data sets put into the Pokedex. The hash function used in the program was to first mutiply the character of the key by 50, then add all of these multiplied characters together. This final sum was then computed to the index number of the hash table by using the mudulus operator to 37. The reason why the character value was multiplied by 50 is to increase the spread of the sum of the keyword, before using the modulo operator to reduce collisions as much as possible. If the hash table was modified to a different algorithm, the spread could easily be increased and thus, the number of collisions would decrease. From this, we would have to traverse less when removing or retrieving, because the chain lengths are lowered. The major efficiency losses in this program were through the areas where full traversal of the chain was necessary, such as when reading from the file into the hash table, writing the table back to the file, and displaying the table of entries. 
If I had more time, I would like to try exploring how to implement a similar design with different data structures, and finding how to get around some of the limitations of certain data structures. I would also like to add more functionality to the Pokedex, like keeping track of pokemon caught, or prioritizing the regions by an area specified by the user.